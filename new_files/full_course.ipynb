{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4bd330",
   "metadata": {},
   "source": [
    "#### **DataFrames y Series**\n",
    "\n",
    "- **DataFrames** -> pd.DataFrame()\n",
    "  - Estructura de datos bidimensional (varias columnas) en pandas.\n",
    "  - Cada columna puede tener un tipo de dato diferente (números, cadenas, fechas, etc.).\n",
    "  - Se pueden crear a partir de diccionarios, listas, archivos CSV, entre otros\n",
    "\n",
    "- **Series** -> pd.Series()\n",
    "  - Estructura de datos unidimensional (una sola columna) en pandas.\n",
    "  - Similar a una columna de un DataFrame.\n",
    "  - Puede contener cualquier tipo de dato y tiene un índice asociado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197cb5e",
   "metadata": {},
   "source": [
    "#### **Operaciones con DataFrames** - Modificación de datos\n",
    "\n",
    "Importamos pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797d206",
   "metadata": {},
   "source": [
    "#### **Reading CSV with Pandas**\n",
    "\n",
    "- CSV is very heavy to work with.\n",
    "- There are other file formats that are more efficient, but CSV is very common and easy to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('../data/results.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26591f4c",
   "metadata": {},
   "source": [
    "#### **Reading PARQUET with Pandas**\n",
    "\n",
    "- PARQUET is a columnar storage file format that is more efficient than CSV.\n",
    "- It is optimized for performance and storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"../data/results.parquet\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4935268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a CSV - no optimo\n",
    "data_csv = data.to_csv(\"data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb243d",
   "metadata": {},
   "source": [
    "#### **SAMPLE - Obtener una muestra aleatoria de un DataFrame**\n",
    "- **df.sample()** permite obtener una muestra aleatoria de un DataFrame.\n",
    "  - _n_: indicar el numero de filas a seleccionar.\n",
    "  - _frac_: indicar la fraccion de filas a seleccionar (no se puede usar junto a n).\n",
    "  - _random_state_: indicar el valor para asegurar reproducibilidad en la seleccion aleatoria.\n",
    "  - _replace_: si es True, permite seleccionar la misma fila mas de una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = data.sample(random_state=10, frac=0.001, replace=False)\n",
    "\n",
    "random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491fc9b7",
   "metadata": {},
   "source": [
    "#### **LOC (location) - Accesing data from file**\n",
    "\n",
    "- **df.loc()** permite acceder a filas y columnas por etiquetas (nombres de filas y columnas)\n",
    "\n",
    "- **df.iloc()** permite acceder a filas y columnas por indices (numeros de filas y columnas) solamente\n",
    "\n",
    "#### **IAT (Index At) - Acceding a single row from a file**\n",
    "\n",
    "- **df.iat()** permite acceder a un solo valor de una fila y columna especifica por indice (numero de fila y numero de columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d329933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a la fila 300000 (index) hasta la 300003 (inclusive)\n",
    "data.loc[300000:300003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a la fila 0, 5 y 10\n",
    "data.loc[[0, 5, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a284b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a las primeras 10 filas\n",
    "data.loc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751dbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder desde la fila 10 hasta el final\n",
    "data.loc[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b086966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a todas las filas solo de la columna \"discipline\"\n",
    "data.loc[510:530, \"discipline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a las primeras 10 filas de las columnas \"discipline\" y \"year\"\n",
    "data.loc[:9, [\"discipline\", \"year\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a las primeras 10 filas de las columnas en las posiciones 2 y 0\n",
    "data.iloc[:10, [2, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar filas por año mayor a 2020 mostrando solo 2 columnas\n",
    "data.loc[data[\"year\"] > 2020, [\"year\", \"as\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar filas por año mayor a 2018 y disciplina \"Tennis\" mostrando solo 2 columnas\n",
    "data.loc[(data[\"year\"] > 2018) & (data[\"discipline\"] == \"Tennis\"), [\"year\", \"discipline\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67db0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva columa \"age\" \n",
    "data[\"age\"] = 2026 - data[\"year\"]\n",
    "data.loc[:50, [\"year\", \"age\"]].sort_values(by=\"age\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generando un DataFrame de 10 filas ordenadas por la columna \"place\" en orden descendente\n",
    "data_piece = data.sort_values(by=[\"place\"], ascending=False).sample(n=10)\n",
    "\n",
    "# Reemplazando los valores NaN en la columna \"place\" por 0\n",
    "data_piece.loc[data_piece[\"place\"].isna(), \"place\"] = 0\n",
    "\n",
    "# Accediendo a la columna \"place\" del DataFrame generado\n",
    "data_piece.loc[:, [\"as\", \"place\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificar el valor de la columna \"year\" en las primeras 10 filas a 2000\n",
    "# Esto modifica el dataframe original\n",
    "data.loc[:10, [\"year\"]] = 2000\n",
    "data.loc[:15, [\"year\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6fed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder al valor de la fila 0 y columna 0\n",
    "data.iat[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe35700",
   "metadata": {},
   "source": [
    "#### **sort_values() - Sort DataFrame by the values of one or more columns.**\n",
    "\n",
    "- **df.sort_values()** permite ordenar un DataFrame por los valores de una o mas columnas.\n",
    "  - _ascending:_ True, forma ascendente. False, forma descendente.\n",
    "  - _by:_ para indicar las columnas por las que se quiere ordenar.\n",
    "  - _inplace:_ True, para modificar el DataFrame original en lugar de crear una copia ordenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f1d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar el DataFrama por las columnas \"year\" (descendente) y \"as\" (ascendente) sin modificar el archivo\n",
    "data_sorted = data.sort_values(by=[\"year\", \"as\"], ascending=[False, True], inplace=False)\n",
    "data_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96438310",
   "metadata": {},
   "source": [
    "#### **Iterar sobre filas de un DataFrame**\n",
    "\n",
    "- No es recomendable iterar sobre las filas de un DataFrame, ya que es ineficiente y va en contra del paradigma de pandas.\n",
    "- Se puede usar el metodo iterrows() para iterar sobre las filas de un DataFrame.\n",
    "  - Cada fila se devuelve como una tupla que contiene el indice de la fila y una Serie con los datos de la fila.\n",
    "- Se puede usar el metodo itertuples() para iterar sobre las filas de un DataFrame.\n",
    "  - Cada fila se devuelve como una tupla nombrada, donde los nombres de los campos son los nombres de las columnas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre las primeras 15 filas de un DataFrame\n",
    "for index, row in data.head(15).iterrows():\n",
    "    print(f\"Index: {index}, Year: {row['year']}, Discipline: {row['discipline']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d1ee1",
   "metadata": {},
   "source": [
    "#### **DROP - Eliminar filas o columnas de un DataFrame**\n",
    "\n",
    "- **df.drop()** permite eliminar filas o columnas de un DataFrame.\n",
    "  - columns=[...] para eliminar columnas por nombre.\n",
    "  - index=[...] para eliminar filas por indice.\n",
    "  - axis=0 para eliminar filas (por defecto) o axis=1 para eliminar columnas.\n",
    "  - inplace=True para modificar el DataFrame original en lugar de crear una copia modificada.\n",
    "  - errors='ignore' para evitar errores si la fila o columna no existe.\n",
    "\n",
    "- **df.dropna()** elimina filas o columnas con valores NaN.\n",
    "  - _axis:_ especifica si eliminar filas (0) o columnas (1).\n",
    "  - _how:_ \n",
    "    - _'any'_ elimina si hay al menos un NaN.\n",
    "    - _'all'_ elimina solo si todos son NaN.\n",
    "  - _thresh:_ numero minimo de valores no-NaN requeridos para mantener la fila/columna.\n",
    "  - _inplace:_ si es True, modifica el DataFrame original en lugar de crear una copia modificada.\n",
    "\n",
    "- **df.drop_duplicates()** elimina filas duplicadas en un DataFrame.\n",
    "  - _subset:_ lista de columnas para considerar al identificar duplicados.\n",
    "  - _keep:_ 'first' (por defecto) mantiene la primera ocurrencia, 'last' mantiene la ultima, False elimina todas las duplicadas.\n",
    "  - _inplace:_ si es True, modifica el DataFrame original en lugar de crear una copia modificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fbe876",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_parquet(\"../data/results.parquet\")\n",
    "new_data.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la columna \"tied\" sin modificar el DataFrame original\n",
    "new_data.drop(columns=[\"tied\"], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.loc[new_data[\"place\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a96c4",
   "metadata": {},
   "source": [
    "#### **Copy() - Crear una copia de un DataFrame**\n",
    "\n",
    "- Pandas utiliza referencias a los datos originales para optimizar el uso de memoria.\n",
    "- Para crear una copia independiente de un DataFrame, se puede usar el metodo copy().\n",
    "- Esto es util cuando se quiere modificar un DataFrame sin afectar al original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a945c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mismo espacio de memoria\n",
    "new_data = data\n",
    "\n",
    "# Eliminar la columna \"year\" modificando el DataFrame original\n",
    "new_data.drop(columns=[\"year\"], inplace=True)\n",
    "\n",
    "new_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferente espacio de memoria\n",
    "new_data = data.copy()\n",
    "\n",
    "# Eliminar la columna \"year\" sin modificar el DataFrame original\n",
    "new_data.drop(columns=[\"year\"], inplace=True)\n",
    "\n",
    "new_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f551b04",
   "metadata": {},
   "source": [
    "#### **Rename y Replace - Renombrar filas / columas y reemplazar valores**\n",
    "\n",
    "- **df.rename()** permite renombrar filas o columnas de un DataFrame.\n",
    "  - _columns={...}_ para renombrar columnas por nombre.\n",
    "  - _index={...}_ para renombrar filas por indice.\n",
    "  - _inplace=True_ para modificar el DataFrame original en lugar de crear una copia modificada.\n",
    "\n",
    "- **df.replace()** permite reemplazar valores en un DataFrame.\n",
    "  - first argument is the value to be replaced.\n",
    "  - second argument is the value to replace with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a316723",
   "metadata": {},
   "outputs": [],
   "source": [
    "bios = pd.read_csv(\"../data/bios.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombramos la columna \"as\" a \"name\" y \"discipline\" a \"sport\" sin modificar el DataFrame original\n",
    "new_data.rename(columns={\"as\": \"name\", \"discipline\": \"sport\"}, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec58a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazamos el valor 180 en la columna \"height_cm\" por 190\n",
    "bios[\"height_cm\"] = bios[\"height_cm\"].replace(180, 190)\n",
    "\n",
    "# Mostramos 5 filas donde la altura es 190 cm\n",
    "bios[bios[\"height_cm\"] == 190].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824f2c1",
   "metadata": {},
   "source": [
    "#### **to_datetime y dt - Acceder a propiedades de fechas en una columna datetime**\n",
    "\n",
    "- **pd.to_datetime()** permite convertir una columna a tipo datetime.\n",
    "  - _format:_ especifica el formato de la fecha en la columna original.\n",
    "  - _errors:_ especifica como manejar errores en la conversion ('raise', 'coerce', 'ignore').\n",
    "  - _utc:_ si es True, convierte las fechas a UTC.\n",
    "- **df.dt.option** permite acceder a propiedades de fechas en una columna datetime.\n",
    "  - Se puede acceder a propiedades como year, month, day, hour, minute, second, weekday, etc.\n",
    "  - Se puede usar para crear nuevas columnas basadas en propiedades de fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c22502",
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_data = pd.read_csv(\"../data/bios.csv\")\n",
    "bios_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna \"born_date\" a tipo datetime\n",
    "bios_data[\"born_date\"] = pd.to_datetime(bios_data[\"born_date\"])\n",
    "# bios_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7961f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder al año\n",
    "bios_data[\"born_year\"] = bios_data[\"born_date\"].dt.year\n",
    "\n",
    "# Acceder al mes\n",
    "bios_data[\"born_month\"] = bios_data[\"born_date\"].dt.month\n",
    "\n",
    "# Acceder al día\n",
    "bios_data[\"born_day\"] = bios_data[\"born_date\"].dt.day\n",
    "\n",
    "bios_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754646c",
   "metadata": {},
   "source": [
    "#### **Apply - Aplicar una funcion a lo largo de un eje de un DataFrame**\n",
    "\n",
    "- **df.apply()** permite aplicar una funcion a lo largo de un eje de un DataFrame (filas o columnas).\n",
    "  - _axis=0_ aplica la funcion a cada columna (por defecto).\n",
    "  - _axis=1_ aplica la funcion a cada fila.\n",
    "  - La funcion puede ser una funcion definida por el usuario o una funcion lambda.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva columna \"weight_class\" basada en la columna \"weight_kg\" usando apply() y una funcion lambda\n",
    "bios_data[\"weight_class\"] = bios_data[\"weight_kg\"].apply(lambda x: \"Heavyweight\" if x >= 100 else (\"Normalweight\" \n",
    "if x >= 70 else \"Lightweight\"))\n",
    "\n",
    "bios_data.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva columna \"height_class\" basada en la columna \"height_cm\" usando apply() y una funcion definida por el usuario\n",
    "def height_class(row):\n",
    "  if row[\"height_cm\"] >= 190:\n",
    "    return \"Tall\"\n",
    "  elif row[\"height_cm\"] >= 170 and row[\"height_cm\"] < 190:\n",
    "    return \"Average\"\n",
    "  else :\n",
    "    return \"Short\"\n",
    "\n",
    "# Aplicar la funcion a lo largo de las filas (axis=1)\n",
    "bios_data[\"height_class\"] = bios_data.apply(height_class, axis=1)\n",
    "\n",
    "bios_data.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb477a",
   "metadata": {},
   "source": [
    "#### **merge y concat - Combinar dos DataFrames basados en una o mas columnas clave**\n",
    "\n",
    "- **df.merge()** permite combinar dos DataFrames basados en una o mas columnas clave.\n",
    "  - _on:_ especifica las columnas clave comunes en ambos DataFrames.\n",
    "  - _left_on:_ especifica las columnas clave en el DataFrame izquierdo.\n",
    "  - _right_on:_ especifica las columnas clave en el DataFrame derecho.\n",
    "  - _how:_ especifica el tipo de combinacion ('inner', 'outer', 'left', 'right').\n",
    "    - _inner:_ solo filas con claves coincidentes en ambos DataFrames.\n",
    "    - _outer:_ todas las filas de ambos DataFrames, con NaN donde no hay coincidencia.\n",
    "    - _left:_ todas las filas del DataFrame izquierdo, con NaN donde no hay coincidencia en el derecho.\n",
    "    - _right:_ todas las filas del DataFrame derecho, con NaN donde no hay coincidencia en el izquierdo.\n",
    "\n",
    "- **pd.concat()** permite concatenar dos o mas DataFrames a lo largo de un eje (filas o columnas).\n",
    "  - _axis=0_ concatena a lo largo de las filas (por defecto). Es como si apilaras los DataFrames uno encima del otro.\n",
    "  - _axis=1_ concatena a lo largo de las columnas. Es como si pusieras los DataFrames uno al lado del otro.\n",
    "  - _ignore_index=True_ para reindexar el DataFrame resultante.\n",
    "  - _join='inner'_ para realizar una interseccion de columnas (por defecto).\n",
    "  - _join='outer'_ para realizar una union de columnas.\n",
    "  - _keys=[...]_ para crear un MultiIndex en el DataFrame resultante.\n",
    "  - _levels=[...]_ para especificar los niveles del MultiIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "750b9a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>athlete_id</th>\n",
       "      <th>name</th>\n",
       "      <th>born_date</th>\n",
       "      <th>born_city</th>\n",
       "      <th>born_region</th>\n",
       "      <th>born_country</th>\n",
       "      <th>NOC</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>died_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [athlete_id, name, born_date, born_city, born_region, born_country, NOC, height_cm, weight_kg, died_date]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noc_regions = pd.read_csv(\"../data/noc_regions.csv\")\n",
    "bios_data = pd.read_csv(\"../data/bios.csv\")\n",
    "bios_data.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e82a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "noc_reduced = noc_regions[[\"region\", \"NOC\"]]\n",
    "new_bios_data = pd.merge(bios_data, noc_reduced, left_on=\"born_country\", right_on=\"NOC\", how=\"inner\")\n",
    "new_bios_data.drop(columns=[\"NOC_y\"], inplace=True)\n",
    "new_bios_data.rename(columns={\"region\": \"born_country_name\", \"NOC_x\": \"NOC\"}, inplace=True)\n",
    "new_bios_data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "067b2c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>athlete_id</th>\n",
       "      <th>name</th>\n",
       "      <th>born_date</th>\n",
       "      <th>born_city</th>\n",
       "      <th>born_region</th>\n",
       "      <th>born_country</th>\n",
       "      <th>NOC</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>died_date</th>\n",
       "      <th>region</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jean-François Blanchy</td>\n",
       "      <td>1886-12-12</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>Gironde</td>\n",
       "      <td>FRA</td>\n",
       "      <td>France</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1960-10-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Arnaud Boetsch</td>\n",
       "      <td>1969-04-01</td>\n",
       "      <td>Meulan</td>\n",
       "      <td>Yvelines</td>\n",
       "      <td>FRA</td>\n",
       "      <td>France</td>\n",
       "      <td>183.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Jean Borotra</td>\n",
       "      <td>1898-08-13</td>\n",
       "      <td>Biarritz</td>\n",
       "      <td>Pyrénées-Atlantiques</td>\n",
       "      <td>FRA</td>\n",
       "      <td>France</td>\n",
       "      <td>183.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1994-07-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Jacques Brugnon</td>\n",
       "      <td>1895-05-11</td>\n",
       "      <td>Paris VIIIe</td>\n",
       "      <td>Paris</td>\n",
       "      <td>FRA</td>\n",
       "      <td>France</td>\n",
       "      <td>168.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1978-03-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Albert Canet</td>\n",
       "      <td>1878-04-17</td>\n",
       "      <td>Wandsworth</td>\n",
       "      <td>England</td>\n",
       "      <td>GBR</td>\n",
       "      <td>France</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1930-07-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145495</th>\n",
       "      <td>149222</td>\n",
       "      <td>Polina Luchnikova</td>\n",
       "      <td>2002-01-30</td>\n",
       "      <td>Serov</td>\n",
       "      <td>Sverdlovsk</td>\n",
       "      <td>RUS</td>\n",
       "      <td>ROC</td>\n",
       "      <td>167.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145496</th>\n",
       "      <td>149223</td>\n",
       "      <td>Valeriya Merkusheva</td>\n",
       "      <td>1999-09-20</td>\n",
       "      <td>Moskva (Moscow)</td>\n",
       "      <td>Moskva</td>\n",
       "      <td>RUS</td>\n",
       "      <td>ROC</td>\n",
       "      <td>168.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145497</th>\n",
       "      <td>149224</td>\n",
       "      <td>Yuliya Smirnova</td>\n",
       "      <td>1998-05-08</td>\n",
       "      <td>Kotlas</td>\n",
       "      <td>Arkhangelsk</td>\n",
       "      <td>RUS</td>\n",
       "      <td>ROC</td>\n",
       "      <td>163.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145498</th>\n",
       "      <td>149225</td>\n",
       "      <td>André Foussard</td>\n",
       "      <td>1899-05-19</td>\n",
       "      <td>Niort</td>\n",
       "      <td>Deux-Sèvres</td>\n",
       "      <td>FRA</td>\n",
       "      <td>France</td>\n",
       "      <td>166.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1986-03-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145499</th>\n",
       "      <td>149814</td>\n",
       "      <td>Bill Phillips</td>\n",
       "      <td>1913-07-15</td>\n",
       "      <td>Dulwich Hill</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-10-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145500 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        athlete_id                   name   born_date        born_city  \\\n",
       "0                1  Jean-François Blanchy  1886-12-12         Bordeaux   \n",
       "1                2         Arnaud Boetsch  1969-04-01           Meulan   \n",
       "2                3           Jean Borotra  1898-08-13         Biarritz   \n",
       "3                4        Jacques Brugnon  1895-05-11      Paris VIIIe   \n",
       "4                5           Albert Canet  1878-04-17       Wandsworth   \n",
       "...            ...                    ...         ...              ...   \n",
       "145495      149222      Polina Luchnikova  2002-01-30            Serov   \n",
       "145496      149223    Valeriya Merkusheva  1999-09-20  Moskva (Moscow)   \n",
       "145497      149224        Yuliya Smirnova  1998-05-08           Kotlas   \n",
       "145498      149225         André Foussard  1899-05-19            Niort   \n",
       "145499      149814          Bill Phillips  1913-07-15     Dulwich Hill   \n",
       "\n",
       "                 born_region born_country        NOC  height_cm  weight_kg  \\\n",
       "0                    Gironde          FRA     France        NaN        NaN   \n",
       "1                   Yvelines          FRA     France      183.0       76.0   \n",
       "2       Pyrénées-Atlantiques          FRA     France      183.0       76.0   \n",
       "3                      Paris          FRA     France      168.0       64.0   \n",
       "4                    England          GBR     France        NaN        NaN   \n",
       "...                      ...          ...        ...        ...        ...   \n",
       "145495            Sverdlovsk          RUS        ROC      167.0       61.0   \n",
       "145496                Moskva          RUS        ROC      168.0       65.0   \n",
       "145497           Arkhangelsk          RUS        ROC      163.0       55.0   \n",
       "145498           Deux-Sèvres          FRA     France      166.0        NaN   \n",
       "145499       New South Wales          AUS  Australia        NaN        NaN   \n",
       "\n",
       "         died_date region notes  \n",
       "0       1960-10-02    NaN   NaN  \n",
       "1              NaN    NaN   NaN  \n",
       "2       1994-07-17    NaN   NaN  \n",
       "3       1978-03-20    NaN   NaN  \n",
       "4       1930-07-25    NaN   NaN  \n",
       "...            ...    ...   ...  \n",
       "145495         NaN    NaN   NaN  \n",
       "145496         NaN    NaN   NaN  \n",
       "145497         NaN    NaN   NaN  \n",
       "145498  1986-03-18    NaN   NaN  \n",
       "145499  2003-10-20    NaN   NaN  \n",
       "\n",
       "[145500 rows x 12 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(bios_data, noc_regions, on=\"NOC\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cbfbf",
   "metadata": {},
   "source": [
    "#### **Not Asigned Values Functions**\n",
    "\n",
    "- **df.isna()** devuelve el DataFrame con valores booleanos indicando si el valor es NaN.\n",
    "\n",
    "- **df.isnull()** es un alias de df.isna() y funciona de la misma manera.\n",
    "  - Se puede utilizar junto con df.sum() para contar el numero de valores NaN en cada columna.\n",
    "\n",
    "- **df.notna()** devuelve el DataFrame con valores booleanos indicando si el valor no es NaN.\n",
    "\n",
    "- **df.notnull()** es un alias de df.notna() y funciona de la misma manera.\n",
    "  - Se puede utilizar junto con df.sum() para contar el numero de valores no NaN en cada columna.\n",
    "\n",
    "- **df.fillna()** permite rellenar valores NaN con un valor especifico\n",
    "  - _value:_ el valor con el que se rellenaran los NaN.\n",
    "  - _inplace:_ si es True, modifica el DataFrame original en lugar de crear una copia modificada.\n",
    "  - _method:_ metodo de relleno.\n",
    "    - _'ffill'_ rellena el valor con el valor anterior.\n",
    "    - _'bfill'_ rellena el valor con el valor siguiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = pd.read_csv(\"../data/coffee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de113305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar los valores NaN en la columna \"Units Sold\" con el promedio de la columna\n",
    "coffee.fillna(value=0, inplace=True)\n",
    "\n",
    "coffee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78702769",
   "metadata": {},
   "source": [
    "#### **Useful functions**\n",
    "\n",
    "- **df.mean()** calcula el promedio de los valores numéricos de una columna (ignora valores NaN).\n",
    "  - _skipna:_ si es True (por defecto), ignora los valores NaN en el calculo.\n",
    "  - _numeric_only:_ si es True, solo considera columnas numericas.\n",
    "\n",
    "- **df.median()** calcula la mediana de los valores numéricos.\n",
    "\n",
    "- **df.min()** calcula el valor minimo de una columna.\n",
    "  - _skipna:_ si es True (por defecto), ignora los valores NaN en el calculo.\n",
    "  - _numeric_only:_ si es True, solo considera columnas numericas.\n",
    "\n",
    "- **df.max()** calcula el valor maximo de una columna.\n",
    "  - _skipna:_ si es True (por defecto), ignora los valores NaN en el calculo.\n",
    "  - _numeric_only:_ si es True, solo considera columnas numericas.\n",
    "\n",
    "- **df.sum()** calcula la suma de los valores numéricos de una columna.\n",
    "  - _skipna:_ si es True (por defecto), ignora los valores NaN en el calculo.\n",
    "  - _numeric_only:_ si es True, solo considera columnas numericas.\n",
    "\n",
    "- **df.cumsum()** calcula la suma acumulativa de los valores a lo largo de un eje.\n",
    "  - _axis:_ especifica el eje a lo largo del cual se aplica la suma acumulativa (0 para filas, 1 para columnas).\n",
    "\n",
    "- **df.value_counts()** cuenta la frecuencia de valores únicos en una columna.\n",
    "\n",
    "- **df.unique()** devuelve los valores únicos en una columna.\n",
    "\n",
    "- **df.duplicated()** devuelve las filas duplicadas en un DataFrame.\n",
    "\n",
    "- **interpolate()** permite rellenar valores NaN mediante interpolación (patrones de valores).\n",
    "  - _method:_ especifica el metodo de interpolacion ('linear', 'time', 'index', 'values', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic').\n",
    "  - _limit:_ especifica el numero maximo de valores NaN a rellenar.\n",
    "  - _inplace:_ si es True, modifica el DataFrame original en lugar de crear una copia modificada.\n",
    "  - _axis:_ especifica el eje a lo largo del cual se aplica la interpolacion (0 para filas, 1 para columnas).\n",
    "\n",
    "- **pd.groupby()** permite agrupar un DataFrame por una o mas columnas y aplicar funciones de agregacion.\n",
    "  - _by:_ especifica las columnas por las que se quiere agrupar.\n",
    "  - _aggfunc:_ especifica la funcion de agregacion a aplicar (por ejemplo, 'mean', 'sum', 'count', etc.).\n",
    "\n",
    "- **df.agg()** permite aplicar una o mas funciones de agregacion a un DataFrame agrupado.\n",
    "  - Se puede pasar un diccionario para aplicar diferentes funciones a diferentes columnas.\n",
    "    - _Ejemplo_: df.agg({'col1': 'mean', 'col2': ['sum', 'max']})\n",
    "  - _func:_ especifica la funcion o lista de funciones de agregacion a aplicar.\n",
    "  - _axis:_ especifica el eje a lo largo del cual se aplica la agregacion (0 para filas, 1 para columnas).\n",
    "\n",
    "- **df.pivot()** permite reorganizar un DataFrame creando una tabla pivote.\n",
    "  - _index:_ especifica las columnas que se utilizaran como indice (filas) en la tabla pivote.\n",
    "  - _columns:_ especifica las columnas que se utilizaran como columnas en la tabla pivote.\n",
    "  - _values:_ especifica las columnas cuyos valores se agregaran en la tabla pivote.\n",
    "  - _aggfunc:_ especifica la funcion de agregacion a aplicar a los valores (por defecto es 'mean').\n",
    "\n",
    "- **df.shift()** permite desplazar los valores de un DataFrame hacia arriba o hacia abajo.\n",
    "  - _periods:_ especifica el numero de periodos a desplazar (positivo -> abajo, negativo -> arriba).\n",
    "  - _axis:_ especifica el eje a lo largo del cual se aplica el desplazamiento (0 para filas, 1 para columnas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = pd.read_csv(\"../warmup-data/coffee.csv\")\n",
    "bios = pd.read_csv(\"../data/bios.csv\")\n",
    "results = pd.read_parquet(\"../data/results.parquet\")\n",
    "bios[\"born_date\"] = pd.to_datetime(bios[\"born_date\"])\n",
    "bios[\"died_date\"] = pd.to_datetime(bios[\"died_date\"])\n",
    "coffee[\"price\"] = coffee[\"Coffee Type\"].map({\"Espresso\": 3.0, \"Latte\": 4.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar los valores NaN en la columna \"Units Sold\" con el promedio de la columna\n",
    "coffee.fillna(coffee[\"Units Sold\"].mean(), inplace=True)\n",
    "\n",
    "coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar las filas donde el tipo de café es \"Espresso\"\n",
    "coffee_espresso = coffee.loc[coffee[\"Coffee Type\"] == \"Espresso\"]\n",
    "\n",
    "# Calcular el promedio de la columna \"Units Sold\" para los cafés tipo \"Espresso\"\n",
    "coffee_espresso[\"Units Sold\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener cuantos cafes se vendieron por dia (ordenado numericamente)\n",
    "coffee[\"Day\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos las personas que nacieron en USA\n",
    "usa_bios = bios[bios[\"born_country\"] == \"USA\"]\n",
    "\n",
    "# Calculamos el promedio de la altura en cm redondeado a 2 decimales\n",
    "usa_bios[\"height_cm\"].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar los valores NaN en la columna \"height_cm\" usando interpolación lineal\n",
    "bios[\"height_cm\"].fillna(bios[\"height_cm\"].interpolate(), inplace=True)\n",
    "\n",
    "# Calcular el promedio de altura por país de nacimiento, redondeado a 2 decimales, y mostrar una muestra aleatoria de 5 países\n",
    "bios.groupby([\"born_country\"])[\"height_cm\"].mean().round(2).sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99480683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar los valores NaN en la columna \"weight_kg\" usando interpolación lineal\n",
    "bios[\"weight_kg\"].fillna(bios[\"weight_kg\"].interpolate(), inplace=True)\n",
    "\n",
    "# Calcular el promedio de peso por NOC para las personas nacidas antes del 2000, redondeado a 2 decimales\n",
    "bios[bios[\"born_date\"].dt.year < 2000].groupby(\"NOC\")[\"weight_kg\"].mean().round(2).sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d521b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una tabla pivote con los días como índice, los tipos de café como columnas y las unidades vendidas como valores\n",
    "coffee[\"Units Sold\"] = coffee[\"Units Sold\"].fillna(coffee[\"Units Sold\"].interpolate()).round(2)\n",
    "pivot = coffee.pivot(columns=\"Coffee Type\", index=\"Day\", values=\"Units Sold\")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los años, meses y días de fallecimiento y contamos las ocurrencias\n",
    "bios[\"year_died\"] = bios[\"died_date\"].dt.year\n",
    "bios[\"month_died\"] = bios[\"died_date\"].dt.month\n",
    "bios[\"day_died\"] = bios[\"died_date\"].dt.day\n",
    "\n",
    "# Contar las ocurrencias de fallecimientos por fecha y mostrar los 10 días con menos fallecimientos\n",
    "bios.groupby([\"year_died\", \"month_died\", \"day_died\"])[\"name\"].count().reset_index(name=\"count\").sort_values(by=\"count\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d12ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar los valores NaN en la columna \"Units Sold\" usando interpolación lineal y redondear a 2 decimales\n",
    "coffee[\"Units Sold\"] = coffee[\"Units Sold\"].fillna(coffee[\"Units Sold\"].interpolate()).round(2)\n",
    "\n",
    "# Calcular los ingresos totales por tipo de café, asumiendo que el precio por unidad es: Espresso = $3.0, Latte = $4.0\n",
    "coffee[\"revenue\"] = coffee[\"Units Sold\"] * coffee[\"price\"]\n",
    "\n",
    "# Mover los ingresos 2 filas hacia abajo para visualizar mejor la diferencia de revenue entre dias\n",
    "coffee[\"yesterday_revenue\"] = coffee[\"revenue\"].shift(2)\n",
    "\n",
    "# Calcular la diferencia de ingresos entre el día actual y el día anterior\n",
    "coffee[\"difference_revenue\"] = coffee[\"revenue\"] - coffee[\"yesterday_revenue\"]\n",
    "\n",
    "coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar los valores NaN en la columna \"born_date\" usando interpolación lineal\n",
    "bios[\"born_date\"] = bios[\"born_date\"].fillna(bios[\"born_date\"].interpolate())\n",
    "\n",
    "# Contar los 10 días del mes con más nacimientos\n",
    "bios[\"born_date\"].dt.day.value_counts().head(10).reset_index(name=\"count\").rename(columns={\"born_date\": \"day\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066de684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva columna \"total_revenue\" que contenga la suma acumulativa de los ingresos diarios\n",
    "\n",
    "coffee[\"Units Sold\"] = coffee[\"Units Sold\"].fillna(coffee[\"Units Sold\"].interpolate()).round(2)\n",
    "\n",
    "coffee[\"revenue\"] = coffee[\"Units Sold\"] * coffee[\"price\"]\n",
    "\n",
    "coffee[\"total_revenue\"] = coffee[\"revenue\"].cumsum()\n",
    "\n",
    "coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee[\"Units Sold\"] = coffee[\"Units Sold\"].fillna(coffee[\"Units Sold\"].interpolate()).round(2)\n",
    "coffee[\"revenue\"] = coffee[\"Units Sold\"] * coffee[\"price\"]\n",
    "\n",
    "espresso_mask = coffee[\"Coffee Type\"] == \"Espresso\"\n",
    "latte_mask = coffee[\"Coffee Type\"] == \"Latte\"\n",
    "\n",
    "coffee[\"espresso_revenue\"] = coffee[\"revenue\"].where(espresso_mask).cumsum().ffill()\n",
    "coffee[\"latte_revenue\"] = coffee[\"revenue\"].where(latte_mask).cumsum().ffill()\n",
    "\n",
    "coffee[\"espresso_units_sold\"] = coffee[\"Units Sold\"].where(espresso_mask).cumsum().ffill()\n",
    "coffee[\"latte_units_sold\"] = coffee[\"Units Sold\"].where(latte_mask).cumsum().ffill()\n",
    "\n",
    "coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bios[(bios[\"born_region\"] == \"New Hampshire\") | (bios[\"born_city\"] == \"San Francisco\")][[\"name\", \"born_region\", \"born_city\"]].sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
